---
title: "Resident Production Model"
author: "Caliper Corporation"
date: "March 2, 2021"
output: 
  html_document:
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(dplyr.summarise.inform = FALSE)
options(scipen = 999)

library(MASS)
library(pscl)
library(tidyverse)
library(knitr)
library(kableExtra)
library(mlr)
```

## Introduction

The goal of the resident trip production models in TRMG2 is to determine how
many trips each individual will make in a day. Unlike traditional trip
generation models, attractions are not estimated in this step. Instead, the
destination choice models estimate coefficients on employment as part of the
size term, which function similar to trip attractions.

TRMG2 production models are person-level models. This means that detailed
information about each person (e.g. age), as well as the characteristics of their
household and geographic location, are used together in the prediction.

```{r}
# Zero-Inflated Negative Binomial Distribution:  
# [https://stats.idre.ucla.edu/r/dae/zinb/](https://stats.idre.ucla.edu/r/dae/zinb/)
# 
# > Zero-inflated negative binomial regression is for modeling count variables with excessive zeros and it is usually for overdispersed count outcome variables. Furthermore, theory suggests that the excess zeros are generated by a separate process from the count values and that the excess zeros can be modeled independently.
```

```{r, include=FALSE}
hh_df <- read_csv("data/output/_PRIVATE/survey_processing/hh_processed.csv")
person_df <- read_csv("data/output/_PRIVATE/survey_processing/per_processed.csv")
trips_df <- read_csv("data/output/_PRIVATE/survey_processing/trips_processed.csv")
logsum <- read_csv("data/input/nhb/logsums.csv")
```

```{r aggregate trips, eval=FALSE}
# # Approach 1: use trip weights. Have non-integer counts of trips.
# aggregate_trips <- trips_df %>%
#   filter(tour_type != "H" & homebased == "HB") %>%
#   group_by(hhid, personid, trip_type) %>%
#   summarize(
#     trips = sum(trip_weight_combined) / sum(hh_weight_combined),
#     p_taz = first(p_taz)
#   ) %>%
#   pivot_wider(names_from = trip_type, values_from = trips) %>%
#   mutate(across(everything(), ~ifelse(is.na(.x), 0, .x))) %>%
#   ungroup()

# Approach 2: use household weights and have integer counts of trips.
aggregate_trips <- trips_df %>%
  filter(tour_type != "H" & homebased == "HB") %>%
  group_by(hhid, personid, trip_type) %>%
  summarize(
    trips = n(),
    p_taz = first(p_taz)
  ) %>%
  pivot_wider(names_from = trip_type, values_from = trips) %>%
  mutate(across(everything(), ~ifelse(is.na(.x), 0, .x))) %>%
  ungroup()

est_tbl <- person_df %>%
  left_join(aggregate_trips %>% select(-hhid), by = "personid") %>%
  left_join(hh_df, by = "hhid") %>%
  mutate(across(N_HB_K12_All:W_HB_EK12_All, ~ifelse(is.na(.x), 0, .x))) %>%
  # feature creation
  mutate(
    weight = hh_weight_combined.x,
    oth_ppl = hhsize - 1,
    oth_wrkr = num_workers - is_worker,
    oth_senior = num_seniors - is_senior,
    oth_kids = num_children - is_child,
    N_HB_K12_All = as.integer(N_HB_K12_All)
  ) %>%
  left_join(
    logsum %>% 
      select(
        TAZ,
        access = access_general_sov,
        g_access = access_general_sov,
        n_access = access_nearby_sov,
        e_access = access_employment_sov
      ),
    by = c("p_taz" = "TAZ")
  ) %>%
  group_by(hhid) %>%
  mutate(access = ifelse(is.na(access), mean(access, na.rm = TRUE), access)) %>%
  ungroup() %>%
  mutate(access = ifelse(is.na(access), mean(access, na.rm = TRUE), access))
```

```{r, N_HB_K12_All, eval=FALSE}
formula <- N_HB_K12_All ~ is_child + is_worker + is_senior + num_vehicles + 
  hhsize + access
m0 <- glm(formula, family = poisson, data = est_tbl)
m1 <- glm.nb(
  formula,
  data = est_tbl
)
m2 <- zeroinfl(
  N_HB_K12_All ~ is_worker + is_senior + num_vehicles + hhsize + oth_kids +
    access | is_child,
  data = est_tbl, dist = "negbin"#, weights = weight
)

# This suggests that the negative binomial is much better than the poisson
# pchisq(2 * (logLik(m1) - logLik(m0)), df = 1, lower.tail = FALSE)

# This shows that the zero-inflated neg binomial is better than the neg binomial
# vuong(m1, m2)

summary(m2)
pR2(m2)
cor(est_tbl$N_HB_K12_All, m2$fitted.values) ^ 2
```

```{r, eval=FALSE}
# multinomial logistic regression
# https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/
library(nnet)
mnl_data <- est_tbl %>%
  filter(!(age > 18 & num_children == 0)) %>%
  mutate(N_HB_K12_All = ifelse(N_HB_K12_All > 3, 3, N_HB_K12_All))
model <- multinom(formula, data = mnl_data)

cor(mnl_data$N_HB_K12_All, as.numeric(predict(model, mnl_data))) ^ 2

# The R^2 is so low due the overwhelming number of people who make 0
# trips.
mnl_data %>%
  group_by(N_HB_K12_All) %>%
  summarize(count = n())
```


```{r, N_HB_OME_All, eval=FALSE}
m2 <- zeroinfl(
  N_HB_OME_All ~ is_senior + num_vehicles + oth_ppl + oth_kids +
    access + oth_senior | oth_ppl + num_seniors + num_children,
  data = est_tbl, dist = "negbin"#, weights = weight
)

summary(m2)
pR2(m2)
cor(est_tbl$N_HB_OME_All, m2$fitted.values) ^ 2

plot(est_tbl$N_HB_OME_All, m2$fitted.values)
```

```{r, eval=FALSE}
# install.packages("mlr-org/mlr3")
# install.packages("xgboost")
# install.packages("parallelMap")
library(mlr)
library(parallelMap)
library(plotly)
```

```{r, eval=FALSE}
# Withhold 10% of household and their people
hhids <- unique(est_tbl$hhid)
test_hhids <- sample(hhids, length(hhids) * .1)
train_hhids <- hhids[!hhids %in% test_hhids]

dt_tbl <- est_tbl %>%
  select(
    N_HB_K12_All, age, g_access, n_access, e_access,
    hhid, weight, is_senior, num_vehicles, oth_ppl, oth_kids, oth_senior, num_seniors, 
    num_children
  ) %>%
  # mutate(across(hhid:num_children, as.factor)) %>%
  mutate(across(c(hhid, weight), as.factor)) %>%
  normalizeFeatures(target = "N_HB_K12_All") %>%
  # createDummyFeatures(
  #   target = "N_HB_K12_All",
  #   cols = c(
  #     "is_senior",
  #     "num_vehicles",
  #     "oth_ppl",
  #     "oth_kids",
  #     "oth_senior",
  #     "num_seniors",
  #     "num_children"
  #   )
  # ) %>%
  mutate(weight = as.numeric(as.character(weight)))

# Split into train/test data
test_set <- dt_tbl %>% filter(hhid %in% test_hhids)
test_hhids <- test_set$hhid
test_set$hhid <- NULL
test_weights <- test_set$weight
test_set$weight <- NULL
train_set <- dt_tbl %>% filter(hhid %in% train_hhids)
train_set$hhid <- NULL
train_weights <- train_set$weight
train_set$weight <- NULL

# trainTask <- makeClassifTask(
#   data = train_set, target = "N_HB_K12_All", weights = train_weights)
# testTask <- makeClassifTask(data = test_set, target = "N_HB_K12_All", weights = test_weights)
trainTask <- makeRegrTask(
  data = train_set, target = "N_HB_K12_All", weights = train_weights)
testTask <- makeRegrTask(data = test_set, target = "N_HB_K12_All", weights = test_weights)

# The classes are imbalanced with 0 being the dominant class. Use oversampling.
# table(getTaskTargets(trainTask))
# trainTask <- oversample(trainTask, rate = 2)
```


```{r create-fit-tune xgboost, eval=FALSE}
# Create learner
xgb_learner <- makeLearner(
  "regr.xgboost",
  # predict.type = "prob",
  predict.type = "response",
  par.vals = list(
    # mlr folks noted bug when objective is included (issue created). remove.
    # objective = "multi:softmax",
    # eval_metric = "logloss"
    # eval_metric = "error"
  )
)

# Make a hyper-parameter set for tuning
xgb_params <- makeParamSet(
  # The number of trees in the model (each one built sequentially)
  makeIntegerParam("nrounds", lower = 50, upper = 250),
  # number of splits in each tree
  makeIntegerParam("max_depth", lower = 1, upper = 5),
  # "shrinkage" - prevents overfitting
  makeNumericParam("eta", lower = .1, upper = .5),
  # L2 regularization - prevents overfitting
  makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)

# Create a control for searching the hyper-parameter space
control <- makeTuneControlRandom(maxit = 20)

# Create a resampling plan
resample_desc <- makeResampleDesc("CV", iters = 3)

# Tune the hyper-parameters
# parallelStartMulticore(8)
tuned_params <- tuneParams(
  learner = xgb_learner,
  task = trainTask,
  resampling = resample_desc,
  par.set = xgb_params,
  control = control
)
# parallelStop()

# Create a new learner using the tuned hyper-parameters
xgb_learner_tuned <- setHyperPars(
  learner = xgb_learner,
  par.vals = tuned_params$x
)

# Create model
xgb_model <- train(xgb_learner_tuned, task = trainTask)

# Save the model so that this chunk does not have to be evaluated every
# time the page is knit.
saveRDS(xgb_model, "data/input/resident_productions/xgb_model.RDS")
```

```{r load xgboost model, eval=FALSE}
xgb_model <- readRDS("data/input/resident_productions/xgb_model.RDS")
```

```{r feature importance, eval=FALSE}
# Show feature importance
imp_tbl <- getFeatureImportance(xgb_model)$res %>%
  as.data.frame()

plot_ly() %>%
  add_trace(
    data = imp_tbl, y = ~reorder(variable, importance), x = ~importance, type = "bar",
    orientation = "h"
  ) %>%
  layout(
    title = "K12 Feature Importance",
    yaxis = list(title = NA),
    xaxis = list(title = "Importance"),
    margin = list(l = 110)
  )

# Make prediction on test data set. Importantly, this will return
# the probabilities as well as set the class the the one with highest
# probability. Ignore that 
xgb_result <- predict(xgb_model, testTask)

# Also get confusion matrix
# conf_mtx <- calculateConfusionMatrix(xgb_result)
# conf_mtx
# measureF1(xgb_result$data$truth, xgb_result$data$response, positive = "1")

# for regression:

# check r^2 at household level
check <- xgb_result$data %>%
  mutate(hhid = test_hhids) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(truth),
    y_hat = sum(response)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
```

```{r, eval=FALSE}
# for the third test (holding back 10% of households and their persons from
# person-level training), collapse back to households
hh_result <- tibble(
  hhid = test_hhids,
  response = xgb_result$data$response
)
```


## Using DT to guide segmentation

After attempting various forms of generalized linear models (GLM) and logistic
regression models, Caliper opted to use Decision Trees (DT) from the machine learning
stack. This model form had much stronger predictive power compared to
traditional approaches while still being easy to understand and explain.

```{r}
#TODO: we can add prose that links to the TRB paper when/if it is published
```

```{r, include=FALSE}
# install.packages("rpart")
# install.packages("rpart.plot")
library(rpart)
library(rpart.plot)

rate_list <- list()
```

```{r}
create_rule_table <- function(tree) {
  rules <- rpart.rules(tree, digits = 4)
  colnames(rules) <- c(
    "rate",
    paste0("c", seq(2:ncol(rules) - 1))
  )
  
  result <- rules %>%
    as_tibble() %>%
    unite(col = "rule", -rate, sep = " ", na.rm = TRUE) %>%
    mutate(
      rate = round(as.numeric(rate), 2),
      rule = gsub("\\s+", " ", rule),
      rule = gsub("&", "and", rule),
      rule = gsub("when ", "", rule, fixed = TRUE),
      rule = gsub(" or ", ",", rule, fixed = TRUE),
      rule = str_replace(rule, "(\\w+)\\sis\\s(\\d+\\.*\\d*)\\sto\\s(\\d+\\.*\\d*)", "\\1 >= \\2 and \\1 < \\3"),
      rule = gsub(" is ", " = ", rule, fixed = TRUE),
      rate = as.numeric(rate)
    )
  return(result)
}
```

```{r}
# # Approach 1: use trip weights. Have non-integer counts of trips.
# aggregate_trips <- trips_df %>%
#   filter(tour_type != "H" & homebased == "HB") %>%
#   group_by(hhid, personid, trip_type) %>%
#   summarize(
#     trips = sum(trip_weight_combined) / sum(hh_weight_combined),
#     p_taz = first(p_taz)
#   ) %>%
#   pivot_wider(names_from = trip_type, values_from = trips) %>%
#   mutate(across(everything(), ~ifelse(is.na(.x), 0, .x))) %>%
#   ungroup()

# Approach 2: use household weights and have integer counts of trips.
aggregate_trips <- trips_df %>%
  filter(tour_type != "H" & homebased == "HB") %>%
  group_by(hhid, personid, trip_type) %>%
  summarize(
    hhweight = first(hh_weight_combined),
    trips = n(),
    p_taz = first(p_taz)
  ) %>%
  pivot_wider(names_from = trip_type, values_from = trips) %>%
  mutate(across(everything(), ~ifelse(is.na(.x), 0, .x))) %>%
  ungroup()

est_tbl <- person_df %>%
  left_join(aggregate_trips %>% select(-hhid), by = "personid") %>%
  left_join(hh_df, by = "hhid") %>%
  mutate(
    p_taz = ifelse(is.na(p_taz), G2_TAZ, p_taz),
    across(N_HB_K12_All:W_HB_EK12_All, ~ifelse(is.na(.x), 0, .x))
  ) %>%
  # feature creation
  mutate(
    weight = hh_weight_combined.x,
    oth_ppl = hhsize - 1,
    oth_wrkr = num_workers - is_worker,
    oth_senior = num_seniors - is_senior,
    oth_kids = num_children - is_child
  ) %>%
  left_join(
    logsum %>%
      transmute(
        TAZ = TAZ, 
        g_access = access_general_sov,
        n_access = access_nearby_sov,
        e_access = access_employment_sov
      ),
    by = c("p_taz" = "TAZ")
  ) %>%
  # group_by(hhid) %>%
  # mutate(
  #   g_access = ifelse(is.na(g_access), mean(g_access, na.rm = TRUE), g_access),
  #   n_access = ifelse(is.na(n_access), mean(n_access, na.rm = TRUE), n_access),
  #   e_access = ifelse(is.na(e_access), mean(e_access, na.rm = TRUE), e_access)
  # ) %>%
  # ungroup() %>%
  mutate(
    # g_access = ifelse(is.na(g_access), mean(g_access, na.rm = TRUE), g_access),
    # n_access = ifelse(is.na(n_access), mean(n_access, na.rm = TRUE), n_access),
    # e_access = ifelse(is.na(e_access), mean(e_access, na.rm = TRUE), e_access),
    # g_access = round(g_access, 1),
    # n_access = round(n_access, 1),
    # e_access = round(e_access, 1),
    retired_hh = ifelse(num_seniors == hhsize & num_workers == 0, 1, 0),
    is_worker = ifelse(retired_hh == 1, 0, is_worker),
    per_inc = hh_income_midpt / hhsize,
    single_parent = ifelse(
      is_child == 0 & num_adults + num_seniors == 1 & num_children > 0, 1, 0)
  )
```

In the trees below, each node lists the average trip rate as well as the percent
of the population that node represents. This lets you see the overall average
trip rate (top of the tree) and how it changes as you segment the surveyed
population.

To save room in the charts, many of the explanatory variables are abbreviated.
Their meanings are listed below for reference:

- Person-level variables
  - is_worker: if the person is a worker
  - is_senior: if the person is >= 65
  - is_child: if the person is < 18
  - age: person's age
  - gender: Male (1) and Female (2)
  - single_parent: if the person is the only adult in a household with children
- Household Variables
  - retired_hh: if the household contains only retirees
  - per_inc: per-capita income (household income / size)
  - oth_ppl: number of other people in the household
  - oth_kids: number of other children in the household
- Zonal variables (see [accessibility page](accessibility.html) for more details)
  - g_access: general accessibility of the person's home zone
  - n_access: nearby accessibility of the person's home zone
  - e_access: employment accessibility of the person's home zone

## Work Tours

### W_HB_W_All

These are the traditional home-based work (HBW) trips. Unsurprisingly, the most
important factor is whether or not the individual is a worker. After that, the
person's age and living arrangements (e.g. other workers) best predict the trip
rate.

```{r}
dt_tbl <- est_tbl %>%
  select(
    W_HB_W_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  W_HB_W_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 5,
  cp = .001,
  minbucket = 30,
  weights = dt_tbl$weight
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
tree <- snip.rpart(tree, 6)
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(W_HB_W_All, weight), 2),
    stdev = round(sd(W_HB_W_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "W_HB_W_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$W_HB_W_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = W_HB_W_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

### W_HB_O_All

When someone makes a stop on their way to or from work, they are making a
home-based other trip on a work tour. Work status is the most important predictor
followed by age. Unlike W_HB_W trips, accessibility of the home zone also plays
a role in determining the frequency of these extra stops. In the bottom right
of the tree, workers that live in areas with lower accessibility make more
of these intermediate stops on work tours. This is the expected behavior. People
living in rural areas are more likely to make these extra stops while out on
work tours.

```{r}
dt_tbl <- est_tbl %>%
  select(
    W_HB_O_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  W_HB_O_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 4,
  cp = .001,
  minbucket = 30,
  weights = dt_tbl$weight
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, c(12))
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(W_HB_O_All, weight), 2),
    stdev = round(sd(W_HB_O_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "W_HB_O_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$W_HB_O_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = W_HB_O_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

### W_HB_EK12_All

This trip type describes adults who drop off children on the way to work or
pick them up on the way home (school escort trips). Having children and being
a worker are both required to qualify to make this trip, and they show up as
the most important questions in the decision tree. Finally, the age of the adult
plays a small role in determining final trip rates.

```{r}
dt_tbl <- est_tbl %>%
  select(
    W_HB_EK12_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent, 
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  W_HB_EK12_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 4,
  cp = .001,
  minbucket = 30,
  weights = dt_tbl$weight
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, 2)
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(W_HB_EK12_All, weight), 2),
    stdev = round(sd(W_HB_EK12_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "W_HB_EK12_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$W_HB_EK12_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = W_HB_EK12_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

## Non-Work Tours

### N_HB_K12_All

This trip type is made by two types of people:

- Students traveling to school
- Parents taking their kids to school (not on a work tour)

The decision tree model recognizes this basic structure in the data and uses
age to split people into these two groups. For adults (left side of the tree),
having kids is required to make this trip. Their work status and age further
refine their trip rate. For children (right side of the tree), age is the primary
factor. Children under 5 do not attend K12, but they do make these trips if they
have older siblings.

```{r}
dt_tbl <- est_tbl %>%
  select(
    N_HB_K12_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  N_HB_K12_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 4,
  cp = .001,
  minbucket = 30,
  weights = dt_tbl$weight
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, 14)
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(N_HB_K12_All, weight), 2),
    stdev = round(sd(N_HB_K12_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "N_HB_K12_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$N_HB_K12_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = N_HB_K12_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

### N_HB_OME_All

This trip type ("Other - Maintenance/Eat" or "OME") captures shopping, dining,
and maintenance activities. These are trips where the primary purpose includes
spending money. These trips are harder to predict based on the data collected in
the survey, and this results in lower r-squared values. Nonetheless, factors
like age, accessibility, work status, and income do help to differentiate trip
rates.

```{r}
dt_tbl <- est_tbl %>%
  select(
    N_HB_OME_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  N_HB_OME_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 4,
  cp = .002,
  minbucket = 30,
  weights = dt_tbl$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, 13)
rpart.plot(tree, type = 1, under = TRUE, extra = 1, cex = .5)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(N_HB_OME_All, weight), 2),
    stdev = round(sd(N_HB_OME_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "N_HB_OME_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$N_HB_OME_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = N_HB_OME_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

### N_HB_OD_Long

These are other, discretionary trips with a long duration time (over 30
minutes). A common example is visiting the home of friends or family. Work
status, age, and accessibility are the primary factors used to differentiate
trip rates.

```{r}
dt_tbl <- est_tbl %>%
  select(
    N_HB_OD_Long, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access,
    per_inc, num_vehicles
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  N_HB_OD_Long ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 5,
  cp = .002,
  minbucket = 30,
  weights = dt_tbl$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, c(12))
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(N_HB_OD_Long, weight), 2),
    stdev = round(sd(N_HB_OD_Long, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "N_HB_OD_Long") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$N_HB_OD_Long <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = N_HB_OD_Long,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

```{r, eval=FALSE}
# multi-tree approach
dt_tbl1 <- est_tbl %>%
  select(
    N_HB_OD_Long, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree1 <- rpart(
  N_HB_OD_Long ~ . - weight,
  data = dt_tbl1, 
  method = "anova",
  maxdepth = 4,
  cp = .0015,
  minbucket = 30,
  weights = dt_tbl1$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree1) # to find node numbers
# tree1 <- snip.rpart(tree1, c(7, 13))
rpart.plot(tree1, type = 1, under = TRUE, extra = 1, cex = .5)

dt_tbl2 <- dt_tbl1 %>%
  select(-is_worker)

tree2 <- rpart(
  N_HB_OD_Long ~ . - weight,
  data = dt_tbl2, 
  method = "anova",
  maxdepth = 4,
  cp = .0015,
  minbucket = 30,
  weights = dt_tbl2$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree2) # to find node numbers
# tree <- snip.rpart(tree2, c(7, 13))
rpart.plot(tree2, type = 1, under = TRUE, extra = 1, cex = .5)

dt_tbl3 <- dt_tbl2 %>%
  select(-age)

tree3 <- rpart(
  N_HB_OD_Long ~ . - weight,
  data = dt_tbl3, 
  method = "anova",
  maxdepth = 4,
  cp = .0015,
  minbucket = 30,
  weights = dt_tbl3$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree3) # to find node numbers
# tree <- snip.rpart(tree3, c(7, 13))
rpart.plot(tree3, type = 1, under = TRUE, extra = 1, cex = .5)

test <- dt_tbl1 %>%
  select(N_HB_OD_Long, weight,) %>%
  mutate(
    tree1 = predict(tree1, dt_tbl1),
    tree2 = predict(tree2, dt_tbl2),
    tree3 = predict(tree3, dt_tbl3)
  )

reg_model <- lm(N_HB_OD_Long ~ tree1 + tree2 + tree3, data = test, weights = weight)
summary(reg_model)

check <- est_tbl %>%
  mutate(
    y = N_HB_OD_Long,
    y_hat = .70329 * predict(tree1, dt_tbl1) + .35910 * predict(tree2, dt_tbl1) +
      .56580 * predict(tree3, dt_tbl3) - .33003
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
round(cor(check$y, check$y_hat)^2, 3)

# One last thing to try: a tree to pick which tree to use
class_tree_tbl <- test %>%
  mutate(
    tree1_err = abs(tree1 - N_HB_OD_Long),
    tree2_err = abs(tree2 - N_HB_OD_Long),
    tree3_err = abs(tree3 - N_HB_OD_Long),
    best_tree = case_when(
      tree1_err < tree2_err & tree1_err < tree3_err ~ "tree1",
      tree2_err < tree1_err & tree2_err < tree3_err ~ "tree2",
      TRUE ~ "tree1"
    )
  )

class_tree <- rpart(
  best_tree ~ tree1 + tree2 + tree3,
  data = class_tree_tbl,
  method = "class",
  maxdepth = 4,
  cp = .0015,
  minbucket = 30,
  weights = class_tree_tbl$weight,
)
rpart.plot(class_tree, type = 1, under = TRUE, extra = 1)

class_test <- test %>%
  mutate(
    tree = predict(class_tree, class_tree_tbl, type = "class"),
    y_hat = case_when(
      tree == "tree1" ~ tree1,
      tree == "tree2" ~ tree2,
      TRUE ~ tree3
    )
  )
```

### N_HB_OD_Short

These are other, discretionary trips with a short activity duration. These are
similar to N_HB_OD_Long, and include things like trips to a friend's house,
church, or other locations. Like other discretionary tours, the low r-squared
indicates some difficulty in making predictions. However, of the variables
available, accessibility, age, income, and children in the household contribute
the most to differences in trip making.

```{r}
dt_tbl <- est_tbl %>%
  select(
    N_HB_OD_Short, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  N_HB_OD_Short ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 5,
  cp = .0015,
  minbucket = 30,
  weights = dt_tbl$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, c(30))
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(N_HB_OD_Short, weight), 2),
    stdev = round(sd(N_HB_OD_Short, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "N_HB_OD_Short") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$N_HB_OD_Short <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = N_HB_OD_Short,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

### N_HB_OMED_All

These are medical trips, and as a result, retirement and age play an important
role in predicting trip rates. This trip type has the poorest r-squared
regardless of model form used. The survey exploratory data analysis shows that
other aspects of this trip type are unique enough to keep it separated - even
this model applied the total average trip rate of 0.066 trips to every person.
The final model below does provide a small improvement over that approach.

```{r}
dt_tbl <- est_tbl %>%
  select(
    N_HB_OMED_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  N_HB_OMED_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 4,
  cp = .002,
  minbucket = 30,
  weights = dt_tbl$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
tree <- snip.rpart(tree, c(7, 13))
rpart.plot(tree, type = 1, under = TRUE, extra = 1, cex = .5)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(N_HB_OMED_All, weight), 2),
    stdev = round(sd(N_HB_OMED_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "N_HB_OMED_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$N_HB_OMED_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = N_HB_OMED_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

## Calibration

Calibrating the production rates is done to ensure that the final model is
producing the same number of trips per person on average as the survey. One
complicating factor in this comparison is that the survey and model
socio-economic data have different total populations.

- Survey: 1.71 million
- Model: 1.83 million

```{r}
survey_pop <- sum(person_df$hh_weight_combined)
model_pop <- 1826973
pop_factor <- model_pop / survey_pop
```

As a consequence, the survey trip totals will be increased by 
`r round(pop_factor, 2)` and then compared to model results. The table below shows
the production model results compared back to the total trips in the survey
(factored up). While close, the model consistently predicts fewer trips compared
to the survey.

```{r, include=FALSE}
# Importantly, this file must be from a model run without any calibration
# factors applied.
model_trips <- read_csv("data/input/resident_productions/modeled_trips_uncalibrated.csv")
```

```{r}
calibration_trips <- trips_df %>%
  filter(tour_type != "H" & homebased == "HB") %>%
  group_by(trip_type) %>%
  summarize(obs_trips = sum(hh_weight_combined) * pop_factor) %>%
  left_join(model_trips, by = "trip_type") %>%
  mutate(
    diff = model_trips - obs_trips,
    `pct_diff` = round(diff / obs_trips * 100, 1)
  ) %>%
  ungroup()

total <- calibration_trips %>%
  summarize(
    trip_type = "Total",
    obs_trips = sum(obs_trips),
    model_trips = sum(model_trips)
  ) %>%
  mutate(
    diff = model_trips - obs_trips,
    `pct_diff` = round(diff / obs_trips * 100, 1)
  )

total <- bind_rows(calibration_trips, total)

total %>%
  rename(
    `Trip Type` = trip_type,
    Observed = obs_trips,
    Modelled = model_trips,
    Difference = diff,
    `%Difference` = pct_diff
  ) %>%
  kable(digits = 0, format.args = list(big.mark = ",")) %>%
  kable_styling(full_width = FALSE)
```

The calibration factors below are the ratio of the observed to modeled trips.
This are applied to each trip type to ensure total trip making is correct.

```{r}
calib_factors <- calibration_trips %>%
  mutate(factor = round(obs_trips / model_trips, 4)) %>%
  select(trip_type, factor)

calib_factors %>%
  rename(`Trip Type` = trip_type, Factor = factor) %>%
  kable(digits = 2) %>%
  kable_styling(full_width = FALSE)
```


```{r, eval=FALSE}
# For the TRB paper, a cross-class model was estimated for N_HB_K12_All
xclass <- est_tbl %>% 
  mutate(
    num_children = ifelse(num_children > 2, 2, num_children),
    age_group = case_when(
      age < 5 ~ "Small Child",
      age < 19 ~ "School Age",
      TRUE ~ "Adult"
    )
  ) %>%
  group_by(age_group, num_children) %>%
  summarize(
    count = n(),
    rate = weighted.mean(N_HB_K12_All, weight = hh_weight_combined.x, na.rm = TRUE)
  ) %>%
  mutate(
    rate = ifelse(age_group == "School Age" & num_children == 0, 0, rate),
    count = ifelse(age_group == "School Age" & num_children == 0, 0, count)
  )

xclass %>%
  select(-count) %>%
  pivot_wider(names_from = num_children, values_from = rate)

xclass %>%
  select(-rate) %>%
  pivot_wider(names_from = num_children, values_from = count)

xclass_predict <- est_tbl %>%
  mutate(
    num_children = ifelse(num_children > 2, 2, num_children),
    age_group = case_when(
      age < 5 ~ "Small Child",
      age < 19 ~ "School Age",
      TRUE ~ "Adult"
    )
  ) %>%
  left_join(xclass, by = c("age_group", "num_children"))
  
cor(xclass_predict$N_HB_K12_All, xclass_predict$rate) ^ 2
```


```{r, eval=FALSE}
out_df <- bind_rows(rate_list) %>%
  group_by(trip_type, rule, rate) %>%
  slice(1) %>%
  mutate(rule = gsub("2,99", "2", rule, fixed = TRUE)) %>%
  select(-category)
write_csv(out_df, "../master/resident/generation/production_rates.csv")

write_csv(
  calib_factors,
  "../master/resident/generation/calibration_factors.csv"
)
```

