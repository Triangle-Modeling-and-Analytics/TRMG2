---
title: "Resident Production Model"
author: "Caliper Corporation"
date: "March 2, 2021"
output: 
  html_document:
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(dplyr.summarise.inform = FALSE)
options(scipen = 999)

library(MASS)
library(pscl)
library(tidyverse)
library(knitr)
library(kableExtra)
library(mlr)
```

## Introduction

The goal of the resident trip production models in TRMG2 is to determine how
many trips each individual will make in a day. Unlike traditional trip
generation models, attractions are not estimated in this step. Instead, the
destination choice models estimate coefficients on employment as part of the
size term, which function similar to trip attractions.

TRMG2 production models are person-level models. This means that detailed
information about each person (e.g. age), as well as the characteristics of their
household and geographic location, are used together in the prediction.

```{r}
# Zero-Inflated Negative Binomial Distribution:  
# [https://stats.idre.ucla.edu/r/dae/zinb/](https://stats.idre.ucla.edu/r/dae/zinb/)
# 
# > Zero-inflated negative binomial regression is for modeling count variables with excessive zeros and it is usually for overdispersed count outcome variables. Furthermore, theory suggests that the excess zeros are generated by a separate process from the count values and that the excess zeros can be modeled independently.
```

```{r, include=FALSE}
hh_df <- read_csv("data/output/_PRIVATE/survey_processing/hh_processed.csv")
person_df <- read_csv("data/output/_PRIVATE/survey_processing/per_processed.csv")
trips_df <- read_csv("data/output/_PRIVATE/survey_processing/trips_processed.csv")
logsum <- read_csv("data/input/nhb/logsums.csv")
```

```{r aggregate trips, eval=FALSE}
# # Approach 1: use trip weights. Have non-integer counts of trips.
# aggregate_trips <- trips_df %>%
#   filter(tour_type != "H" & homebased == "HB") %>%
#   group_by(hhid, personid, trip_type) %>%
#   summarize(
#     trips = sum(trip_weight_combined) / sum(hh_weight_combined),
#     p_taz = first(p_taz)
#   ) %>%
#   pivot_wider(names_from = trip_type, values_from = trips) %>%
#   mutate(across(everything(), ~ifelse(is.na(.x), 0, .x))) %>%
#   ungroup()

# Approach 2: use household weights and have integer counts of trips.
aggregate_trips <- trips_df %>%
  filter(tour_type != "H" & homebased == "HB") %>%
  group_by(hhid, personid, trip_type) %>%
  summarize(
    trips = n(),
    p_taz = first(p_taz)
  ) %>%
  pivot_wider(names_from = trip_type, values_from = trips) %>%
  mutate(across(everything(), ~ifelse(is.na(.x), 0, .x))) %>%
  ungroup()

est_tbl <- person_df %>%
  left_join(aggregate_trips %>% select(-hhid), by = "personid") %>%
  left_join(hh_df, by = "hhid") %>%
  mutate(across(N_HB_K12_All:W_HB_EK12_All, ~ifelse(is.na(.x), 0, .x))) %>%
  # feature creation
  mutate(
    weight = hh_weight_combined.x,
    oth_ppl = hhsize - 1,
    oth_wrkr = num_workers - is_worker,
    oth_senior = num_seniors - is_senior,
    oth_kids = num_children - is_child,
    N_HB_K12_All = as.integer(N_HB_K12_All)
  ) %>%
  left_join(
    logsum %>% 
      select(
        TAZ,
        access = access_general_sov,
        g_access = access_general_sov,
        n_access = access_nearby_sov,
        e_access = access_employment_sov
      ),
    by = c("p_taz" = "TAZ")
  ) %>%
  group_by(hhid) %>%
  mutate(access = ifelse(is.na(access), mean(access, na.rm = TRUE), access)) %>%
  ungroup() %>%
  mutate(access = ifelse(is.na(access), mean(access, na.rm = TRUE), access))
```

```{r, N_HB_K12_All, eval=FALSE}
formula <- N_HB_K12_All ~ is_child + is_worker + is_senior + num_vehicles + 
  hhsize + access
m0 <- glm(formula, family = poisson, data = est_tbl)
m1 <- glm.nb(
  formula,
  data = est_tbl
)
m2 <- zeroinfl(
  N_HB_K12_All ~ is_worker + is_senior + num_vehicles + hhsize + oth_kids +
    access | is_child,
  data = est_tbl, dist = "negbin"#, weights = weight
)

# This suggests that the negative binomial is much better than the poisson
# pchisq(2 * (logLik(m1) - logLik(m0)), df = 1, lower.tail = FALSE)

# This shows that the zero-inflated neg binomial is better than the neg binomial
# vuong(m1, m2)

summary(m2)
pR2(m2)
cor(est_tbl$N_HB_K12_All, m2$fitted.values) ^ 2
```

```{r, eval=FALSE}
# multinomial logistic regression
# https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/
library(nnet)
mnl_data <- est_tbl %>%
  filter(!(age > 18 & num_children == 0)) %>%
  mutate(N_HB_K12_All = ifelse(N_HB_K12_All > 3, 3, N_HB_K12_All))
model <- multinom(formula, data = mnl_data)

cor(mnl_data$N_HB_K12_All, as.numeric(predict(model, mnl_data))) ^ 2

# The R^2 is so low due the overwhelming number of people who make 0
# trips.
mnl_data %>%
  group_by(N_HB_K12_All) %>%
  summarize(count = n())
```


```{r, N_HB_OME_All, eval=FALSE}
m2 <- zeroinfl(
  N_HB_OME_All ~ is_senior + num_vehicles + oth_ppl + oth_kids +
    access + oth_senior | oth_ppl + num_seniors + num_children,
  data = est_tbl, dist = "negbin"#, weights = weight
)

summary(m2)
pR2(m2)
cor(est_tbl$N_HB_OME_All, m2$fitted.values) ^ 2

plot(est_tbl$N_HB_OME_All, m2$fitted.values)
```

```{r, eval=FALSE}
# install.packages("mlr-org/mlr3")
# install.packages("xgboost")
# install.packages("parallelMap")
library(mlr)
library(parallelMap)
library(plotly)
```

```{r, eval=FALSE}
# Withhold 10% of household and their people
hhids <- unique(est_tbl$hhid)
test_hhids <- sample(hhids, length(hhids) * .1)
train_hhids <- hhids[!hhids %in% test_hhids]

dt_tbl <- est_tbl %>%
  select(
    N_HB_K12_All, age, g_access, n_access, e_access,
    hhid, weight, is_senior, num_vehicles, oth_ppl, oth_kids, oth_senior, num_seniors, 
    num_children
  ) %>%
  # mutate(across(hhid:num_children, as.factor)) %>%
  mutate(across(c(hhid, weight), as.factor)) %>%
  normalizeFeatures(target = "N_HB_K12_All") %>%
  # createDummyFeatures(
  #   target = "N_HB_K12_All",
  #   cols = c(
  #     "is_senior",
  #     "num_vehicles",
  #     "oth_ppl",
  #     "oth_kids",
  #     "oth_senior",
  #     "num_seniors",
  #     "num_children"
  #   )
  # ) %>%
  mutate(weight = as.numeric(as.character(weight)))

# Split into train/test data
test_set <- dt_tbl %>% filter(hhid %in% test_hhids)
test_hhids <- test_set$hhid
test_set$hhid <- NULL
test_weights <- test_set$weight
test_set$weight <- NULL
train_set <- dt_tbl %>% filter(hhid %in% train_hhids)
train_set$hhid <- NULL
train_weights <- train_set$weight
train_set$weight <- NULL

# trainTask <- makeClassifTask(
#   data = train_set, target = "N_HB_K12_All", weights = train_weights)
# testTask <- makeClassifTask(data = test_set, target = "N_HB_K12_All", weights = test_weights)
trainTask <- makeRegrTask(
  data = train_set, target = "N_HB_K12_All", weights = train_weights)
testTask <- makeRegrTask(data = test_set, target = "N_HB_K12_All", weights = test_weights)

# The classes are imbalanced with 0 being the dominant class. Use oversampling.
# table(getTaskTargets(trainTask))
# trainTask <- oversample(trainTask, rate = 2)
```


```{r create-fit-tune xgboost, eval=FALSE}
# Create learner
xgb_learner <- makeLearner(
  "regr.xgboost",
  # predict.type = "prob",
  predict.type = "response",
  par.vals = list(
    # mlr folks noted bug when objective is included (issue created). remove.
    # objective = "multi:softmax",
    # eval_metric = "logloss"
    # eval_metric = "error"
  )
)

# Make a hyper-parameter set for tuning
xgb_params <- makeParamSet(
  # The number of trees in the model (each one built sequentially)
  makeIntegerParam("nrounds", lower = 50, upper = 250),
  # number of splits in each tree
  makeIntegerParam("max_depth", lower = 1, upper = 5),
  # "shrinkage" - prevents overfitting
  makeNumericParam("eta", lower = .1, upper = .5),
  # L2 regularization - prevents overfitting
  makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)

# Create a control for searching the hyper-parameter space
control <- makeTuneControlRandom(maxit = 20)

# Create a resampling plan
resample_desc <- makeResampleDesc("CV", iters = 3)

# Tune the hyper-parameters
# parallelStartMulticore(8)
tuned_params <- tuneParams(
  learner = xgb_learner,
  task = trainTask,
  resampling = resample_desc,
  par.set = xgb_params,
  control = control
)
# parallelStop()

# Create a new learner using the tuned hyper-parameters
xgb_learner_tuned <- setHyperPars(
  learner = xgb_learner,
  par.vals = tuned_params$x
)

# Create model
xgb_model <- train(xgb_learner_tuned, task = trainTask)

# Save the model so that this chunk does not have to be evaluated every
# time the page is knit.
saveRDS(xgb_model, "data/input/resident_productions/xgb_model.RDS")
```

```{r load xgboost model, eval=FALSE}
xgb_model <- readRDS("data/input/resident_productions/xgb_model.RDS")
```

```{r feature importance, eval=FALSE}
# Show feature importance
imp_tbl <- getFeatureImportance(xgb_model)$res %>%
  as.data.frame()

plot_ly() %>%
  add_trace(
    data = imp_tbl, y = ~reorder(variable, importance), x = ~importance, type = "bar",
    orientation = "h"
  ) %>%
  layout(
    title = "K12 Feature Importance",
    yaxis = list(title = NA),
    xaxis = list(title = "Importance"),
    margin = list(l = 110)
  )

# Make prediction on test data set. Importantly, this will return
# the probabilities as well as set the class the the one with highest
# probability. Ignore that 
xgb_result <- predict(xgb_model, testTask)

# Also get confusion matrix
# conf_mtx <- calculateConfusionMatrix(xgb_result)
# conf_mtx
# measureF1(xgb_result$data$truth, xgb_result$data$response, positive = "1")

# for regression:

# check r^2 at household level
check <- xgb_result$data %>%
  mutate(hhid = test_hhids) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(truth),
    y_hat = sum(response)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
```

```{r, eval=FALSE}
# for the third test (holding back 10% of households and their persons from
# person-level training), collapse back to households
hh_result <- tibble(
  hhid = test_hhids,
  response = xgb_result$data$response
)
```


## Using DT to guide segmentation

After attempting various forms of generalized linear models (GLM) and logistic
regression models, Caliper opted to use Decision Trees (DT) from the machine learning
stack. This model form had much stronger predictive power compared to
traditional approaches while still being easy to understand and explain.

```{r}
#TODO: we can add prose that links to the TRB paper when/if it is published
```

```{r, include=FALSE}
# install.packages("rpart")
# install.packages("rpart.plot")
library(rpart)
library(rpart.plot)

rate_list <- list()
```

```{r}
create_rule_table <- function(tree) {
  rules <- rpart.rules(tree, digits = 4)
  colnames(rules) <- c(
    "rate",
    paste0("c", seq(2:ncol(rules) - 1))
  )
  
  result <- rules %>%
    as_tibble() %>%
    unite(col = "rule", -rate, sep = " ", na.rm = TRUE) %>%
    mutate(
      rate = round(as.numeric(rate), 2),
      rule = gsub("\\s+", " ", rule),
      rule = gsub("&", "and", rule),
      rule = gsub("when ", "", rule, fixed = TRUE),
      rule = gsub(" or ", ",", rule, fixed = TRUE),
      rule = str_replace(rule, "(\\w+)\\sis\\s(\\d+\\.*\\d*)\\sto\\s(\\d+\\.*\\d*)", "\\1 >= \\2 and \\1 < \\3"),
      rule = gsub(" is ", " = ", rule, fixed = TRUE),
      rate = as.numeric(rate)
    )
  return(result)
}
```

```{r}
# # Approach 1: use trip weights. Have non-integer counts of trips.
# aggregate_trips <- trips_df %>%
#   filter(tour_type != "H" & homebased == "HB") %>%
#   group_by(hhid, personid, trip_type) %>%
#   summarize(
#     trips = sum(trip_weight_combined) / sum(hh_weight_combined),
#     p_taz = first(p_taz)
#   ) %>%
#   pivot_wider(names_from = trip_type, values_from = trips) %>%
#   mutate(across(everything(), ~ifelse(is.na(.x), 0, .x))) %>%
#   ungroup()

# Approach 2: use household weights and have integer counts of trips.
aggregate_trips <- trips_df %>%
  filter(tour_type != "H" & homebased == "HB") %>%
  group_by(hhid, personid, trip_type) %>%
  summarize(
    hhweight = first(hh_weight_combined),
    trips = n(),
    p_taz = first(p_taz)
  ) %>%
  pivot_wider(names_from = trip_type, values_from = trips) %>%
  mutate(across(everything(), ~ifelse(is.na(.x), 0, .x))) %>%
  ungroup()

est_tbl <- person_df %>%
  left_join(aggregate_trips %>% select(-hhid), by = "personid") %>%
  left_join(hh_df, by = "hhid") %>%
  mutate(
    p_taz = ifelse(is.na(p_taz), G2_TAZ, p_taz),
    across(N_HB_K12_All:W_HB_EK12_All, ~ifelse(is.na(.x), 0, .x))
  ) %>%
  # feature creation
  mutate(
    weight = hh_weight_combined.x,
    oth_ppl = hhsize - 1,
    oth_wrkr = num_workers - is_worker,
    oth_senior = num_seniors - is_senior,
    oth_kids = num_children - is_child
  ) %>%
  left_join(
    logsum %>%
      transmute(
        TAZ = TAZ, 
        g_access = access_general_sov,
        n_access = access_nearby_sov,
        e_access = access_employment_sov
      ),
    by = c("p_taz" = "TAZ")
  ) %>%
  # group_by(hhid) %>%
  # mutate(
  #   g_access = ifelse(is.na(g_access), mean(g_access, na.rm = TRUE), g_access),
  #   n_access = ifelse(is.na(n_access), mean(n_access, na.rm = TRUE), n_access),
  #   e_access = ifelse(is.na(e_access), mean(e_access, na.rm = TRUE), e_access)
  # ) %>%
  # ungroup() %>%
  mutate(
    # g_access = ifelse(is.na(g_access), mean(g_access, na.rm = TRUE), g_access),
    # n_access = ifelse(is.na(n_access), mean(n_access, na.rm = TRUE), n_access),
    # e_access = ifelse(is.na(e_access), mean(e_access, na.rm = TRUE), e_access),
    # g_access = round(g_access, 1),
    # n_access = round(n_access, 1),
    # e_access = round(e_access, 1),
    retired_hh = ifelse(num_seniors == hhsize & num_workers == 0, 1, 0),
    is_worker = ifelse(retired_hh == 1, 0, is_worker),
    per_inc = hh_income_midpt / hhsize,
    single_parent = ifelse(
      is_child == 0 & num_adults + num_seniors == 1 & num_children > 0, 1, 0)
  )
```

In the trees below, each node lists the average trip rate as well as the percent
of the population that node represents. This lets you see the overall average
trip rate (top of the tree) and how it changes as you segment the surveyed
population.

To save room in the charts, many of the explanatory variables are abbreviated.
Their meanings are listed below for reference:

- Person-level variables
  - is_worker: if the person is a worker
  - is_senior: if the person is >= 65
  - is_child: if the person is < 18
  - age: person's age
  - gender: Male (1) and Female (2)
  - single_parent: if the person is the only adult in a household with children
- Household Variables
  - retired_hh: if the household contains only retirees
  - per_inc: per-capita income (household income / size)
  - oth_ppl: number of other people in the household
  - oth_kids: number of other children in the household
- Zonal variables (see [accessibility page](accessibility.html) for more details)
  - g_access: general accessibility of the person's home zone
  - n_access: nearby accessibility of the person's home zone
  - e_access: employment accessibility of the person's home zone

## Work Tours

### W_HB_W_All

These are the traditional home-based work (HBW) trips. Unsurprisingly, the most
important factor is whether or not the individual is a worker. After that, the
person's age and living arrangements (e.g. other workers) best predict the trip
rate.

```{r}
dt_tbl <- est_tbl %>%
  select(
    W_HB_W_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  W_HB_W_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 5,
  cp = .001,
  minbucket = 30,
  weights = dt_tbl$weight
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, 6)
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(W_HB_W_All, weight), 2),
    stdev = round(sd(W_HB_W_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "W_HB_W_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$W_HB_W_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = W_HB_W_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

### W_HB_O_All

When someone makes a stop on their way to or from work, they are making a
home-based other trip on a work tour. Work status is the most important predictor
followed by age. The make up of the house is also important. Households with other
people present are less likely to need to make (e.g.) grocery stops on the way
home from work. This is an intuitive result.

```{r}
dt_tbl <- est_tbl %>%
  select(
    W_HB_O_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  W_HB_O_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 4,
  cp = .001,
  minbucket = 30,
  weights = dt_tbl$weight
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, c(12))
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(W_HB_O_All, weight), 2),
    stdev = round(sd(W_HB_O_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "W_HB_O_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$W_HB_O_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = W_HB_O_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

### W_HB_EK12_All

This trip type describes adults who drop off children on the way to work or
pick them up on the way home (school escort trips). Having children and being
a worker are both required to qualify to make this trip, and they show up as
the most important questions in the decision tree. Finally, the age of the adult
plays a small role in determining final trip rates.

```{r}
dt_tbl <- est_tbl %>%
  select(
    W_HB_EK12_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent, 
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  W_HB_EK12_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 4,
  cp = .001,
  minbucket = 30,
  weights = dt_tbl$weight
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, 2)
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(W_HB_EK12_All, weight), 2),
    stdev = round(sd(W_HB_EK12_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "W_HB_EK12_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$W_HB_EK12_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = W_HB_EK12_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

## Non-Work Tours

### N_HB_K12_All

This trip type is made by two types of people:

- Students traveling to school
- Parents taking their kids to school (not on a work tour)

The decision tree model recognizes this basic structure in the data and uses
age to split people into these two groups. For adults (left side of the tree),
having kids is required to make this trip. Their work status and age further
refine their trip rate. For children (right side of the tree), age is the primary
factor. Children under 5 do not attend K12, but they do make these trips if they
have older siblings.

```{r}
dt_tbl <- est_tbl %>%
  select(
    N_HB_K12_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  N_HB_K12_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 4,
  cp = .001,
  minbucket = 30,
  weights = dt_tbl$weight
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, 14)
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(N_HB_K12_All, weight), 2),
    stdev = round(sd(N_HB_K12_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "N_HB_K12_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$N_HB_K12_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = N_HB_K12_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

### N_HB_OME_All

This trip type ("Other - Maintenance/Eat" or "OME") captures shopping, dining,
and maintenance activities. These are trips where the primary purpose includes
spending money. These trips are harder to predict based on the data collected in
the survey, and this results in lower r-squared values. Nonetheless, factors
like age, accessibility, work status, and income do help to differentiate trip
rates.

```{r}
dt_tbl <- est_tbl %>%
  select(
    N_HB_OME_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  N_HB_OME_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 4,
  cp = .002,
  minbucket = 30,
  weights = dt_tbl$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, 13)
rpart.plot(tree, type = 1, under = TRUE, extra = 1, cex = .5)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(N_HB_OME_All, weight), 2),
    stdev = round(sd(N_HB_OME_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "N_HB_OME_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$N_HB_OME_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = N_HB_OME_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

### N_HB_OD_Long

These are other, discretionary trips with a long duration time (over 30
minutes). A common example is visiting the home of friends or family. Work
status, age, and accessibility are the primary factors used to differentiate
trip rates.

```{r}
dt_tbl <- est_tbl %>%
  select(
    N_HB_OD_Long, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access,
    per_inc, num_vehicles
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  N_HB_OD_Long ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 5,
  cp = .002,
  minbucket = 30,
  weights = dt_tbl$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, c(12))
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(N_HB_OD_Long, weight), 2),
    stdev = round(sd(N_HB_OD_Long, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "N_HB_OD_Long") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$N_HB_OD_Long <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = N_HB_OD_Long,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

```{r, eval=FALSE}
# multi-tree approach
dt_tbl1 <- est_tbl %>%
  select(
    N_HB_OD_Long, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree1 <- rpart(
  N_HB_OD_Long ~ . - weight,
  data = dt_tbl1, 
  method = "anova",
  maxdepth = 4,
  cp = .0015,
  minbucket = 30,
  weights = dt_tbl1$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree1) # to find node numbers
# tree1 <- snip.rpart(tree1, c(7, 13))
rpart.plot(tree1, type = 1, under = TRUE, extra = 1, cex = .5)

dt_tbl2 <- dt_tbl1 %>%
  select(-is_worker)

tree2 <- rpart(
  N_HB_OD_Long ~ . - weight,
  data = dt_tbl2, 
  method = "anova",
  maxdepth = 4,
  cp = .0015,
  minbucket = 30,
  weights = dt_tbl2$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree2) # to find node numbers
# tree <- snip.rpart(tree2, c(7, 13))
rpart.plot(tree2, type = 1, under = TRUE, extra = 1, cex = .5)

dt_tbl3 <- dt_tbl2 %>%
  select(-age)

tree3 <- rpart(
  N_HB_OD_Long ~ . - weight,
  data = dt_tbl3, 
  method = "anova",
  maxdepth = 4,
  cp = .0015,
  minbucket = 30,
  weights = dt_tbl3$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree3) # to find node numbers
# tree <- snip.rpart(tree3, c(7, 13))
rpart.plot(tree3, type = 1, under = TRUE, extra = 1, cex = .5)

test <- dt_tbl1 %>%
  select(N_HB_OD_Long, weight,) %>%
  mutate(
    tree1 = predict(tree1, dt_tbl1),
    tree2 = predict(tree2, dt_tbl2),
    tree3 = predict(tree3, dt_tbl3)
  )

reg_model <- lm(N_HB_OD_Long ~ tree1 + tree2 + tree3, data = test, weights = weight)
summary(reg_model)

check <- est_tbl %>%
  mutate(
    y = N_HB_OD_Long,
    y_hat = .70329 * predict(tree1, dt_tbl1) + .35910 * predict(tree2, dt_tbl1) +
      .56580 * predict(tree3, dt_tbl3) - .33003
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
round(cor(check$y, check$y_hat)^2, 3)

# One last thing to try: a tree to pick which tree to use
class_tree_tbl <- test %>%
  mutate(
    tree1_err = abs(tree1 - N_HB_OD_Long),
    tree2_err = abs(tree2 - N_HB_OD_Long),
    tree3_err = abs(tree3 - N_HB_OD_Long),
    best_tree = case_when(
      tree1_err < tree2_err & tree1_err < tree3_err ~ "tree1",
      tree2_err < tree1_err & tree2_err < tree3_err ~ "tree2",
      TRUE ~ "tree1"
    )
  )

class_tree <- rpart(
  best_tree ~ tree1 + tree2 + tree3,
  data = class_tree_tbl,
  method = "class",
  maxdepth = 4,
  cp = .0015,
  minbucket = 30,
  weights = class_tree_tbl$weight,
)
rpart.plot(class_tree, type = 1, under = TRUE, extra = 1)

class_test <- test %>%
  mutate(
    tree = predict(class_tree, class_tree_tbl, type = "class"),
    y_hat = case_when(
      tree == "tree1" ~ tree1,
      tree == "tree2" ~ tree2,
      TRUE ~ tree3
    )
  )
```

### N_HB_OD_Short

These are other, discretionary trips with a short activity duration. These are
similar to N_HB_OD_Long, and include things like trips to a friend's house,
church, or other locations. Like other discretionary tours, the low r-squared
indicates some difficulty in making predictions. However, of the variables
available, accessibility, age, and children in the household contribute
the most to differences in trip making.

```{r}
dt_tbl <- est_tbl %>%
  select(
    N_HB_OD_Short, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  N_HB_OD_Short ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 5,
  cp = .0015,
  minbucket = 30,
  weights = dt_tbl$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
tree <- snip.rpart(tree, c(10, 31))
rpart.plot(tree, type = 1, under = TRUE, extra = 1, cex = .5)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(N_HB_OD_Short, weight), 2),
    stdev = round(sd(N_HB_OD_Short, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "N_HB_OD_Short") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$N_HB_OD_Short <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = N_HB_OD_Short,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

### N_HB_OMED_All

These are medical trips, and as a result, retirement and age play an important
role in predicting trip rates. This trip type has the poorest r-squared
regardless of model form used. The survey exploratory data analysis shows that
other aspects of this trip type are unique enough to keep it separated - even
this model applied the total average trip rate of 0.066 trips to every person.
The final model below does provide a small improvement over that approach.

```{r}
dt_tbl <- est_tbl %>%
  select(
    N_HB_OMED_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  N_HB_OMED_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 4,
  cp = .002,
  minbucket = 30,
  weights = dt_tbl$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
tree <- snip.rpart(tree, c(7, 13))
rpart.plot(tree, type = 1, under = TRUE, extra = 1, cex = .5)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(N_HB_OMED_All, weight), 2),
    stdev = round(sd(N_HB_OMED_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "N_HB_OMED_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$N_HB_OMED_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = N_HB_OMED_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

## Calibration

Calibrating the production rates is done to ensure that the final model is
producing the same number of trips per person on average as the survey. One
complicating factor in this comparison is that the survey and model
socio-economic data have different total populations.

- Survey: 1.71 million
- Model: 1.83 million

```{r}
survey_pop <- sum(person_df$hh_weight_combined)
model_pop <- 1826973
pop_factor <- model_pop / survey_pop
```

As a consequence, the survey trip totals will be increased by 
`r round(pop_factor, 2)` and then compared to model results. The table below shows
the production model results compared back to the total trips in the survey
(factored up). While close, the model consistently predicts fewer trips compared
to the survey.

Another important consideration is which type of survey weight to use. RSG
provides two weight fields on the survey:

- Household weights
- Trip weights

Importantly, trip weights have extra adjustment based on GPS traces to account
for things like un-reported trips. As a result, the trip weight gives a higher
number of total trips. For estimation, however, the decision trees worked best
with household weights, which allowed each person to have an integer count of
trips instead of 1.2 or 3.4. This allowed us to identify the features properly,
but then calibration is required to scale up the trip totals as shown below.

Finally, the market segmentation created by the Auto Ownership model is not
exactly the same as what is observed in the survey. While the share of total
households owning 1, 2, 3, and 4+ autos is calibrated to match the survey, the
AO model cannot guarantee that market segmentation will match. This is because
auto sufficiency is defined by autos owned and number of adults. In other words,
the production rate calibration factors are the first opportunity to correct
market segmentation imbalanced introduced by the auto ownership model.

In the table below, the model results are compared to the survey trips. As
expected (due to the weighting issue above), the model underpredicts trip making.

```{r, include=FALSE}
# Importantly, this file must be from a model run without any calibration
# factors applied.
model_trips <- read_csv(
  "data/input/resident_productions/modeled_trips_uncalibrated.csv") %>%
  pivot_longer(
    N_HB_K12_All:W_HB_W_All, names_to = "trip_type", values_to = "model")
```

```{r}
calibration_trips <- trips_df %>%
  filter(tour_type != "H" & homebased == "HB") %>%
  group_by(trip_type, segment = choice_segment) %>%
  summarize(survey = sum(trip_weight_combined) * pop_factor) %>%
  left_join(model_trips, by = c("trip_type", "segment")) %>%
  mutate(segment2 = case_when(
    segment == "v0" ~ "v0",
    grepl("vi", segment) ~ "vi",
    TRUE ~ "vs"
  )) %>%
  group_by(trip_type, segment = segment2) %>%
    summarize(survey = sum(survey), model = sum(model)) %>%
  mutate(
    diff = model - survey,
    `pct_diff` = round(diff / survey * 100, 1)
  ) %>%
  ungroup()

rollup <- calibration_trips %>%
  group_by(trip_type) %>%
  summarize(
    survey = sum(survey),
    model = sum(model)
  ) %>%
  mutate(
    diff = model - survey,
    `pct_diff` = round(diff / survey * 100, 1)
  ) %>%
  ungroup()

total <- rollup %>%
  summarize(
    trip_type = "All",
    survey = sum(survey),
    model = sum(model)
  ) %>%
  mutate(
    diff = model - survey,
    `pct_diff` = round(diff / survey * 100, 1)
  )

total <- bind_rows(rollup, total)

total %>%
  rename(
    `Trip Type` = trip_type,
    Observed = survey,
    Modelled = model,
    Difference = diff,
    `%Difference` = pct_diff
  ) %>%
  kable(digits = 0, format.args = list(big.mark = ",")) %>%
  kable_styling(full_width = FALSE)
```

The calibration factors below are the ratio of the observed to modeled trips.
These are applied by trip type and segment to ensure total trip making matches
the survey (based on trip weight).

```{r}
calib_factors <- calibration_trips %>%
  mutate(factor = survey / model) %>%
  select(trip_type, segment, factor) %>%
  mutate(
    factor = factor * 0.963, # final validation factor based on counts
    factor = round(factor, 3)
  ) 

calib_factors %>%
  rename(`Trip Type` = trip_type, Factor = factor) %>%
  kable(digits = 2) %>%
  kable_styling(full_width = FALSE)
```


```{r, eval=FALSE}
# For the TRB paper, a cross-class model was estimated for N_HB_K12_All
xclass <- est_tbl %>% 
  mutate(
    num_children = ifelse(num_children > 2, 2, num_children),
    age_group = case_when(
      age < 5 ~ "Small Child",
      age < 19 ~ "School Age",
      TRUE ~ "Adult"
    )
  ) %>%
  group_by(age_group, num_children) %>%
  summarize(
    count = n(),
    rate = weighted.mean(N_HB_K12_All, weight = hh_weight_combined.x, na.rm = TRUE)
  ) %>%
  mutate(
    rate = ifelse(age_group == "School Age" & num_children == 0, 0, rate),
    count = ifelse(age_group == "School Age" & num_children == 0, 0, count)
  )

xclass %>%
  select(-count) %>%
  pivot_wider(names_from = num_children, values_from = rate)

xclass %>%
  select(-rate) %>%
  pivot_wider(names_from = num_children, values_from = count)

xclass_predict <- est_tbl %>%
  mutate(
    num_children = ifelse(num_children > 2, 2, num_children),
    age_group = case_when(
      age < 5 ~ "Small Child",
      age < 19 ~ "School Age",
      TRUE ~ "Adult"
    )
  ) %>%
  left_join(xclass, by = c("age_group", "num_children"))
  
cor(xclass_predict$N_HB_K12_All, xclass_predict$rate) ^ 2
```


```{r, eval=FALSE}
out_df <- bind_rows(rate_list) %>%
  group_by(trip_type, rule, rate) %>%
  slice(1) %>%
  mutate(rule = gsub("2,99", "2", rule, fixed = TRUE)) %>%
  select(-category)
write_csv(out_df, "../master/resident/generation/production_rates.csv")

write_csv(
  calib_factors,
  "../master/resident/generation/calibration_factors.csv"
)
```

