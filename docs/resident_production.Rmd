---
title: "Resident Production Model"
author: "Caliper Corporation"
date: "March 2, 2021"
output: 
  html_document:
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(dplyr.summarise.inform = FALSE)
options(scipen = 999)

library(MASS)
library(pscl)
library(tidyverse)
library(knitr)
library(kableExtra)
library(mlr)
```

## Introduction

*TODO: Add*

- We tried GLM (up to and including zero-inflated negative binomial)
- We tried logistic regression
- We explored ML
- We ended up with decision-tree-assisted cross classification

```{r}
# Zero-Inflated Negative Binomial Distribution:  
# [https://stats.idre.ucla.edu/r/dae/zinb/](https://stats.idre.ucla.edu/r/dae/zinb/)
# 
# > Zero-inflated negative binomial regression is for modeling count variables with excessive zeros and it is usually for overdispersed count outcome variables. Furthermore, theory suggests that the excess zeros are generated by a separate process from the count values and that the excess zeros can be modeled independently.
```

```{r, include=FALSE}
hh_df <- read_csv("data/output/_PRIVATE/survey_processing/hh_processed.csv")
person_df <- read_csv("data/output/_PRIVATE/survey_processing/per_processed.csv")
trips_df <- read_csv("data/output/_PRIVATE/survey_processing/trips_processed.csv")
logsum <- read_csv("data/input/nhb/logsums.csv")
```

```{r aggregate trips, eval=FALSE}
# # Approach 1: use trip weights. Have non-integer counts of trips.
# aggregate_trips <- trips_df %>%
#   filter(tour_type != "H" & homebased == "HB") %>%
#   group_by(hhid, personid, trip_type) %>%
#   summarize(
#     trips = sum(trip_weight_combined) / sum(hh_weight_combined),
#     p_taz = first(p_taz)
#   ) %>%
#   pivot_wider(names_from = trip_type, values_from = trips) %>%
#   mutate(across(everything(), ~ifelse(is.na(.x), 0, .x))) %>%
#   ungroup()

# Approach 2: use household weights and have integer counts of trips.
aggregate_trips <- trips_df %>%
  filter(tour_type != "H" & homebased == "HB") %>%
  group_by(hhid, personid, trip_type) %>%
  summarize(
    trips = n(),
    p_taz = first(p_taz)
  ) %>%
  pivot_wider(names_from = trip_type, values_from = trips) %>%
  mutate(across(everything(), ~ifelse(is.na(.x), 0, .x))) %>%
  ungroup()

est_tbl <- person_df %>%
  left_join(aggregate_trips %>% select(-hhid), by = "personid") %>%
  left_join(hh_df, by = "hhid") %>%
  mutate(across(N_HB_K12_All:W_HB_EK12_All, ~ifelse(is.na(.x), 0, .x))) %>%
  # feature creation
  mutate(
    weight = hh_weight_combined.x,
    oth_ppl = hhsize - 1,
    oth_wrkr = num_workers - is_worker,
    oth_senior = num_seniors - is_senior,
    oth_kids = num_children - is_child,
    N_HB_K12_All = as.integer(N_HB_K12_All)
  ) %>%
  left_join(
    logsum %>% select(TAZ, access = access_general_sov),
    by = c("p_taz" = "TAZ")
  ) %>%
  group_by(hhid) %>%
  mutate(access = ifelse(is.na(access), mean(access, na.rm = TRUE), access)) %>%
  ungroup() %>%
  mutate(access = ifelse(is.na(access), mean(access, na.rm = TRUE), access))
```

```{r, N_HB_K12_All, eval=FALSE}
formula <- N_HB_K12_All ~ is_child + is_worker + is_senior + num_vehicles + 
  hhsize + access
m0 <- glm(formula, family = poisson, data = est_tbl)
m1 <- glm.nb(
  formula,
  data = est_tbl
)
m2 <- zeroinfl(
  N_HB_K12_All ~ is_worker + is_senior + num_vehicles + hhsize + oth_kids +
    access | is_child,
  data = est_tbl, dist = "negbin"#, weights = weight
)

# This suggests that the negative binomial is much better than the poisson
# pchisq(2 * (logLik(m1) - logLik(m0)), df = 1, lower.tail = FALSE)

# This shows that the zero-inflated neg binomial is better than the neg binomial
# vuong(m1, m2)

summary(m2)
pR2(m2)
```

```{r, N_HB_OME_All, eval=FALSE}
m2 <- zeroinfl(
  N_HB_OME_All ~ is_senior + num_vehicles + oth_ppl + oth_kids +
    access + oth_senior | oth_ppl + num_seniors + num_children,
  data = est_tbl, dist = "negbin"#, weights = weight
)

summary(m2)
pR2(m2)
cor(est_tbl$N_HB_OME_All, m2$fitted.values) ^ 2

plot(est_tbl$N_HB_OME_All, m2$fitted.values)
```

```{r, eval=FALSE}
# install.packages("mlr-org/mlr3")
# install.packages("xgboost")
# install.packages("parallelMap")
library(mlr)
library(parallelMap)
library(plotly)
```

```{r, eval=FALSE}
# Withhold 10% of household and their people
hhids <- unique(est_tbl$hhid)
test_hhids <- sample(hhids, length(hhids) * .1)
train_hhids <- hhids[!hhids %in% test_hhids]

dt_tbl <- est_tbl %>%
  select(
    N_HB_OD_Long, age, g_access, n_access, e_access,
    hhid, weight, is_senior, num_vehicles, oth_ppl, oth_kids, oth_senior, num_seniors, 
    num_children
  ) %>%
  mutate(across(hhid:num_children, as.factor)) %>%
  normalizeFeatures(target = "N_HB_OD_Long") %>%
  # createDummyFeatures(
  #   target = "N_HB_OD_Long",
  #   cols = c(
  #     "is_senior",
  #     "num_vehicles",
  #     "oth_ppl",
  #     "oth_kids",
  #     "oth_senior",
  #     "num_seniors",
  #     "num_children"
  #   )
  # ) %>%
  mutate(weight = as.numeric(as.character(weight)))

# Split into train/test data
test_set <- dt_tbl %>% filter(hhid %in% test_hhids)
test_hhids <- test_set$hhid
test_set$hhid <- NULL
test_weights <- test_set$weight
test_set$weight <- NULL
train_set <- dt_tbl %>% filter(hhid %in% train_hhids)
train_set$hhid <- NULL
train_weights <- train_set$weight
train_set$weight <- NULL

# trainTask <- makeClassifTask(
#   data = train_set, target = "N_HB_OD_Long", weights = train_weights)
# testTask <- makeClassifTask(data = test_set, target = "N_HB_OD_Long", weights = test_weights)
trainTask <- makeRegrTask(
  data = train_set, target = "N_HB_OD_Long", weights = train_weights)
testTask <- makeRegrTask(data = test_set, target = "N_HB_OD_Long", weights = test_weights)

# The classes are imbalanced with 0 being the dominant class. Use oversampling.
# table(getTaskTargets(trainTask))
# trainTask <- oversample(trainTask, rate = 2)
```


```{r create-fit-tune xgboost, eval=FALSE}
# Create learner
xgb_learner <- makeLearner(
  "regr.xgboost",
  # predict.type = "prob",
  predict.type = "response",
  par.vals = list(
    # mlr folks noted bug when objective is included (issue created). remove.
    # objective = "multi:softmax",
    # eval_metric = "logloss"
    # eval_metric = "error"
  )
)

# Make a hyper-parameter set for tuning
xgb_params <- makeParamSet(
  # The number of trees in the model (each one built sequentially)
  makeIntegerParam("nrounds", lower = 50, upper = 250),
  # number of splits in each tree
  makeIntegerParam("max_depth", lower = 1, upper = 5),
  # "shrinkage" - prevents overfitting
  makeNumericParam("eta", lower = .1, upper = .5),
  # L2 regularization - prevents overfitting
  makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)

# Create a control for searching the hyper-parameter space
control <- makeTuneControlRandom(maxit = 10)

# Create a resampling plan
resample_desc <- makeResampleDesc("CV", iters = 3)

# Tune the hyper-parameters
parallelStartMulticore(8)
tuned_params <- tuneParams(
  learner = xgb_learner,
  task = trainTask,
  resampling = resample_desc,
  par.set = xgb_params,
  control = control
)
parallelStop()

# Create a new learner using the tuned hyper-parameters
xgb_learner_tuned <- setHyperPars(
  learner = xgb_learner,
  par.vals = tuned_params$x
)

# Create model
xgb_model <- train(xgb_learner_tuned, task = trainTask)

# Save the model so that this chunk does not have to be evaluated every
# time the page is knit.
# saveRDS(xgb_model, "data/output/resident_production/xgb_person_level.RDS")
# saveRDS(xgb_model, "data/output/resident_production/xgb_hh_level.RDS")
saveRDS(xgb_model, "xgb_model.RDS")
```

```{r load xgboost model, eval=FALSE}
# xgb_model <- readRDS("data/output/resident_production/xgb_person_level.RDS")
# xgb_model <- readRDS("data/output/resident_production/xgb_hh_level.RDS")
xgb_model <- readRDS("xgb_model.RDS")
```

```{r feature importance, eval=FALSE}
# Show feature importance
imp_tbl <- getFeatureImportance(xgb_model)
imp_tbl <- imp_tbl$res #%>%
  # gather(key = "Feature", value = "Importance") %>%
  # arrange(Importance)

plot_ly() %>%
  add_trace(
    data = imp_tbl, y = ~variable, x = ~importance, type = "bar",
    orientation = "h"
  ) %>%
  layout(margin = list(l = 110))

# Make prediction on test data set. Importantly, this will return
# the probabilities as well as set the class the the one with highest
# probability. Ignore that 
xgb_result <- predict(xgb_model, testTask)

# Also get confusion matrix
# conf_mtx <- calculateConfusionMatrix(xgb_result)
# conf_mtx
# measureF1(xgb_result$data$truth, xgb_result$data$response, positive = "1")

# for regression:

# check r^2 at household level
check <- xgb_result$data %>%
  mutate(hhid = test_hhids) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(truth),
    y_hat = sum(response)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
```

```{r, eval=FALSE}
# for the third test (holding back 10% of households and their persons from
# person-level training), collapse back to households
hh_result <- tibble(
  hhid = test_hhids,
  response = xgb_result$data$response
)
```


```{r, eval=FALSE}
trainTask <- makeClassifTask(data = train_set, target = "N_HB_OME_All", weights = train_weights)
testTask <- makeClassifTask(data = test_set, target = "N_HB_OME_All", weights = test_weights)


# Create learner
# logreg_learner <- makeLearner("classif.logreg")
logreg_learner <- makeLearner("classif.binomial")

# Create model
logreg_model <- train(logreg_learner, task = trainTask)

logreg_result <- predict(logreg_model, testTask)
conf_mtx <- calculateConfusionMatrix(logreg_result)
conf_mtx
measureF1(logreg_result$data$truth, logreg_result$data$response, positive = "1")
```


## Using DT to guide segmentation

```{r, include=FALSE}
# install.packages("rpart")
# install.packages("rpart.plot")
library(rpart)
library(rpart.plot)

rate_list <- list()
```

```{r}
create_rule_table <- function(tree) {
  rules <- rpart.rules(tree, digits = 4)
  colnames(rules) <- c(
    "rate",
    paste0("c", seq(2:ncol(rules) - 1))
  )
  
  result <- rules %>%
    as_tibble() %>%
    unite(col = "rule", -rate, sep = " ", na.rm = TRUE) %>%
    mutate(
      rate = round(as.numeric(rate), 2),
      rule = gsub("\\s+", " ", rule),
      rule = gsub("&", "and", rule),
      rule = gsub("when ", "", rule, fixed = TRUE),
      rule = gsub(" or ", ",", rule, fixed = TRUE),
      rule = str_replace(rule, "(\\w+)\\sis\\s(\\d+\\.*\\d*)\\sto\\s(\\d+\\.*\\d*)", "\\1 >= \\2 and \\1 < \\3"),
      rule = gsub(" is ", " = ", rule, fixed = TRUE),
      rate = as.numeric(rate)
    )
  return(result)
}
```

```{r}
# # Approach 1: use trip weights. Have non-integer counts of trips.
# aggregate_trips <- trips_df %>%
#   filter(tour_type != "H" & homebased == "HB") %>%
#   group_by(hhid, personid, trip_type) %>%
#   summarize(
#     trips = sum(trip_weight_combined) / sum(hh_weight_combined),
#     p_taz = first(p_taz)
#   ) %>%
#   pivot_wider(names_from = trip_type, values_from = trips) %>%
#   mutate(across(everything(), ~ifelse(is.na(.x), 0, .x))) %>%
#   ungroup()

# Approach 2: use household weights and have integer counts of trips.
aggregate_trips <- trips_df %>%
  filter(tour_type != "H" & homebased == "HB") %>%
  group_by(hhid, personid, trip_type) %>%
  summarize(
    hhweight = first(hh_weight_combined),
    trips = n(),
    p_taz = first(p_taz)
  ) %>%
  pivot_wider(names_from = trip_type, values_from = trips) %>%
  mutate(across(everything(), ~ifelse(is.na(.x), 0, .x))) %>%
  ungroup()

est_tbl <- person_df %>%
  left_join(aggregate_trips %>% select(-hhid), by = "personid") %>%
  left_join(hh_df, by = "hhid") %>%
  mutate(
    p_taz = ifelse(is.na(p_taz), G2_TAZ, p_taz),
    across(N_HB_K12_All:W_HB_EK12_All, ~ifelse(is.na(.x), 0, .x))
  ) %>%
  # feature creation
  mutate(
    weight = hh_weight_combined.x,
    oth_ppl = hhsize - 1,
    oth_wrkr = num_workers - is_worker,
    oth_senior = num_seniors - is_senior,
    oth_kids = num_children - is_child
  ) %>%
  left_join(
    logsum %>%
      transmute(
        TAZ = TAZ, 
        g_access = access_general_sov,
        n_access = access_nearby_sov,
        e_access = access_employment_sov
      ),
    by = c("p_taz" = "TAZ")
  ) %>%
  # group_by(hhid) %>%
  # mutate(
  #   g_access = ifelse(is.na(g_access), mean(g_access, na.rm = TRUE), g_access),
  #   n_access = ifelse(is.na(n_access), mean(n_access, na.rm = TRUE), n_access),
  #   e_access = ifelse(is.na(e_access), mean(e_access, na.rm = TRUE), e_access)
  # ) %>%
  # ungroup() %>%
  mutate(
    # g_access = ifelse(is.na(g_access), mean(g_access, na.rm = TRUE), g_access),
    # n_access = ifelse(is.na(n_access), mean(n_access, na.rm = TRUE), n_access),
    # e_access = ifelse(is.na(e_access), mean(e_access, na.rm = TRUE), e_access),
    # g_access = round(g_access, 1),
    # n_access = round(n_access, 1),
    # e_access = round(e_access, 1),
    retired_hh = ifelse(num_seniors == hhsize & num_workers == 0, 1, 0),
    is_worker = ifelse(retired_hh == 1, 0, is_worker),
    per_inc = hh_income_midpt / hhsize,
    single_parent = ifelse(
      is_child == 0 & num_adults + num_seniors == 1 & num_children > 0, 1, 0)
  )
```

In the trees below, each node lists the average trip rate as well as the percent
of the population that node represents. This lets you see the overall average
trip rate (top of the tree) and how it changes as you segment the surveyed
population.

To save room in the charts, many of the explanatory variables are abbreviated.
Their meanings are listed below for reference:

- Person-level variables
  - is_worker: if the person is a worker
  - is_senior: if the person is >= 65
  - is_child: if the person is < 18
  - age: person's age
  - gender: Male (1) and Female (2)
  - single_parent: if the person is the only adult in a household with children
- Household Variables
  - retired_hh: if the household contains only retirees
  - per_inc: per-capita income (household income / size)
  - oth_ppl: number of other people in the household
  - oth_kids: number of other kids in the household
- Zonal variables
  - g_access: general accessibility of the person's home zone
  - n_access: nearby accessibility of the person's home zone
  - e_access: employment accessibility of the person's home zone

*TODO: Add prose for each chart after they are finalized*

## Work Tours

### Work

W_HB_W_All

```{r}
dt_tbl <- est_tbl %>%
  select(
    W_HB_W_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  W_HB_W_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 5,
  cp = .001,
  minbucket = 30,
  weights = dt_tbl$weight
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
tree <- snip.rpart(tree, 6)
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(W_HB_W_All, weight), 2),
    stdev = round(sd(W_HB_W_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "W_HB_W_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$W_HB_W_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = W_HB_W_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

### Other

W_HB_O_All

```{r}
dt_tbl <- est_tbl %>%
  select(
    W_HB_O_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  W_HB_O_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 4,
  cp = .001,
  minbucket = 30,
  weights = dt_tbl$weight
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, c(12))
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(W_HB_O_All, weight), 2),
    stdev = round(sd(W_HB_O_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "W_HB_O_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$W_HB_O_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = W_HB_O_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

### K12 Drop Off

W_HB_EK12_All

```{r}
dt_tbl <- est_tbl %>%
  select(
    W_HB_EK12_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent, 
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  W_HB_EK12_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 4,
  cp = .001,
  minbucket = 30,
  weights = dt_tbl$weight
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, 2)
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(W_HB_EK12_All, weight), 2),
    stdev = round(sd(W_HB_EK12_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "W_HB_EK12_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$W_HB_EK12_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = W_HB_EK12_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

## Non-Work Tours

### K12

N_HB_K12_All

```{r}
dt_tbl <- est_tbl %>%
  select(
    N_HB_K12_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  N_HB_K12_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 4,
  cp = .001,
  minbucket = 30,
  weights = dt_tbl$weight
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, 14)
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(N_HB_K12_All, weight), 2),
    stdev = round(sd(N_HB_K12_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "N_HB_K12_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$N_HB_K12_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = N_HB_K12_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

### Shopping/Dining

N_HB_OME_All

```{r}
dt_tbl <- est_tbl %>%
  select(
    N_HB_OME_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  N_HB_OME_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 4,
  cp = .002,
  minbucket = 30,
  weights = dt_tbl$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, 13)
rpart.plot(tree, type = 1, under = TRUE, extra = 1, cex = .5)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(N_HB_OME_All, weight), 2),
    stdev = round(sd(N_HB_OME_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "N_HB_OME_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$N_HB_OME_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = N_HB_OME_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

### Other (long duration)

N_HB_OD_Long

```{r}
dt_tbl <- est_tbl %>%
  select(
    N_HB_OD_Long, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access,
    per_inc, num_vehicles
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  N_HB_OD_Long ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 5,
  cp = .002,
  minbucket = 30,
  weights = dt_tbl$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, c(12))
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(N_HB_OD_Long, weight), 2),
    stdev = round(sd(N_HB_OD_Long, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "N_HB_OD_Long") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$N_HB_OD_Long <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = N_HB_OD_Long,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

```{r, eval=FALSE}
# multi-tree approach
dt_tbl1 <- est_tbl %>%
  select(
    N_HB_OD_Long, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree1 <- rpart(
  N_HB_OD_Long ~ . - weight,
  data = dt_tbl1, 
  method = "anova",
  maxdepth = 4,
  cp = .0015,
  minbucket = 30,
  weights = dt_tbl1$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree1) # to find node numbers
# tree1 <- snip.rpart(tree1, c(7, 13))
rpart.plot(tree1, type = 1, under = TRUE, extra = 1, cex = .5)

dt_tbl2 <- dt_tbl1 %>%
  select(-is_worker)

tree2 <- rpart(
  N_HB_OD_Long ~ . - weight,
  data = dt_tbl2, 
  method = "anova",
  maxdepth = 4,
  cp = .0015,
  minbucket = 30,
  weights = dt_tbl2$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree2) # to find node numbers
# tree <- snip.rpart(tree2, c(7, 13))
rpart.plot(tree2, type = 1, under = TRUE, extra = 1, cex = .5)

dt_tbl3 <- dt_tbl2 %>%
  select(-age)

tree3 <- rpart(
  N_HB_OD_Long ~ . - weight,
  data = dt_tbl3, 
  method = "anova",
  maxdepth = 4,
  cp = .0015,
  minbucket = 30,
  weights = dt_tbl3$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree3) # to find node numbers
# tree <- snip.rpart(tree3, c(7, 13))
rpart.plot(tree3, type = 1, under = TRUE, extra = 1, cex = .5)

test <- dt_tbl1 %>%
  select(N_HB_OD_Long, weight,) %>%
  mutate(
    tree1 = predict(tree1, dt_tbl1),
    tree2 = predict(tree2, dt_tbl2),
    tree3 = predict(tree3, dt_tbl3)
  )

reg_model <- lm(N_HB_OD_Long ~ tree1 + tree2 + tree3, data = test, weights = weight)
summary(reg_model)

check <- est_tbl %>%
  mutate(
    y = N_HB_OD_Long,
    y_hat = .70329 * predict(tree1, dt_tbl1) + .35910 * predict(tree2, dt_tbl1) +
      .56580 * predict(tree3, dt_tbl3) - .33003
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
round(cor(check$y, check$y_hat)^2, 3)

# One last thing to try: a tree to pick which tree to use
class_tree_tbl <- test %>%
  mutate(
    tree1_err = abs(tree1 - N_HB_OD_Long),
    tree2_err = abs(tree2 - N_HB_OD_Long),
    tree3_err = abs(tree3 - N_HB_OD_Long),
    best_tree = case_when(
      tree1_err < tree2_err & tree1_err < tree3_err ~ "tree1",
      tree2_err < tree1_err & tree2_err < tree3_err ~ "tree2",
      TRUE ~ "tree1"
    )
  )

class_tree <- rpart(
  best_tree ~ tree1 + tree2 + tree3,
  data = class_tree_tbl,
  method = "class",
  maxdepth = 4,
  cp = .0015,
  minbucket = 30,
  weights = class_tree_tbl$weight,
)
rpart.plot(class_tree, type = 1, under = TRUE, extra = 1)

class_test <- test %>%
  mutate(
    tree = predict(class_tree, class_tree_tbl, type = "class"),
    y_hat = case_when(
      tree == "tree1" ~ tree1,
      tree == "tree2" ~ tree2,
      TRUE ~ tree3
    )
  )
```

### Other (short duration)

N_HB_OD_Short

```{r}
dt_tbl <- est_tbl %>%
  select(
    N_HB_OD_Short, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access, per_inc
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  N_HB_OD_Short ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 5,
  cp = .0015,
  minbucket = 30,
  weights = dt_tbl$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
# tree <- snip.rpart(tree, c(30))
rpart.plot(tree, type = 1, under = TRUE, extra = 1)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(N_HB_OD_Short, weight), 2),
    stdev = round(sd(N_HB_OD_Short, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "N_HB_OD_Short") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$N_HB_OD_Short <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = N_HB_OD_Short,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

### Medical

N_HB_OMED_All

```{r}
dt_tbl <- est_tbl %>%
  select(
    N_HB_OMED_All, weight,
    is_senior, is_child, is_worker, gender, retired_hh, single_parent,
    oth_ppl, oth_kids, oth_senior, oth_wrkr, age, g_access, n_access, e_access
  ) %>%
  mutate(across(is_senior:single_parent, as.factor))

tree <- rpart(
  N_HB_OMED_All ~ . - weight,
  data = dt_tbl, 
  method = "anova",
  maxdepth = 4,
  cp = .002,
  minbucket = 30,
  weights = dt_tbl$weight,
)

# Snip to remove specific nodes from the tree
# summary(tree) # to find node numbers
tree <- snip.rpart(tree, c(7, 13))
rpart.plot(tree, type = 1, under = TRUE, extra = 1, cex = .5)

rule_tbl <- create_rule_table(tree)
rate_tbl <- dt_tbl %>%
  mutate(category = tree$where,) %>%
  group_by(category) %>%
  dplyr::summarize(
    rate = round(weighted.mean(N_HB_OMED_All, weight), 2),
    stdev = round(sd(N_HB_OMED_All, na.rm = TRUE), 2),
    samples = n()
  ) %>%
  ungroup() %>%
  mutate(trip_type = "N_HB_OMED_All") %>%
  left_join(rule_tbl, by = "rate") %>%
  relocate(trip_type:rule, .before = category) %>%
  arrange(rate)
rate_list$N_HB_OMED_All <- rate_tbl

# check r^2 at household level
check <- est_tbl %>%
  mutate(
    y = N_HB_OMED_All,
    y_hat = predict(tree, dt_tbl)
  ) %>%
  group_by(hhid) %>%
  summarize(
    y = sum(y),
    y_hat = sum(y_hat)
  )
r_sq <- round(cor(check$y, check$y_hat)^2, 3)
# rsq.rpart(tree)
```

R-squared: `r r_sq`

## Calibration

The table below shows the production model results compared back to the total
trips in the survey. Overall, the model performs extremely well even without
calibration.

```{r, include=FALSE}
model_trips <- read_csv("data/input/resident_productions/modeled_trips_uncalibrated.csv")
```

```{r}
calibration_trips <- trips_df %>%
  filter(tour_type != "H" & homebased == "HB") %>%
  group_by(trip_type) %>%
  summarize(obs_trips = sum(hh_weight_combined)) %>%
  left_join(model_trips, by = "trip_type") %>%
  mutate(
    diff = model_trips - obs_trips,
    `pct_diff` = round(diff / obs_trips * 100, 1)
  ) %>%
  ungroup()

total <- calibration_trips %>%
  summarize(
    trip_type = "Total",
    obs_trips = sum(obs_trips),
    model_trips = sum(model_trips)
  ) %>%
  mutate(
    diff = model_trips - obs_trips,
    `pct_diff` = round(diff / obs_trips * 100, 1)
  )

calibration_trips <- bind_rows(calibration_trips, total)

calibration_trips %>%
  rename(
    `Trip Type` = trip_type,
    Observed = obs_trips,
    Modelled = model_trips,
    Difference = diff,
    `%Difference` = pct_diff
  ) %>%
  kable(digits = 0, format.args = list(big.mark = ",")) %>%
  kable_styling(full_width = FALSE)
```


```{r, eval=FALSE}
out_df <- bind_rows(rate_list) %>%
  group_by(trip_type, rule, rate) %>%
  slice(1) %>%
  mutate(rule = gsub("2,99", "2", rule, fixed = TRUE)) %>%
  select(-category)
write_csv(out_df, "../master/resident/generation/production_rates.csv")
```

